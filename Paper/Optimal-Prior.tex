%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Preamble %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Word target: 2,031 words 

% Outline:
% Abstract: 92 words
% I. Introduction: 454 words
% II. GCE estimator: 375 words
% III. MP-GCE estimator: 392 words
% IV. Sampling Experiments: 327 words
% V. Results: 390 words
% VI. Conclusions: 93 words

% Declare document class and miscellaneous packages
\documentclass{elsarticle}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{ctable}
\usepackage{setspace}
\usepackage{longtable}
\usepackage{url}
\usepackage{moredefs,lips} 
\usepackage{IEEEtrantools}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage[normalsize]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage[all]{nowidow}
\usepackage{listings}
\usepackage{graphicx}
\urlstyle{rm}

%Hyper-references
\usepackage{hyperref}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black, 
urlcolor=black, pdftex}

% Begin document and frontmatter
\begin{document}
\begin{frontmatter}

% Title
\title{Incorporating Prior Information When True Priors are Unknown: An 
Information-Theoretic Approach for Increasing Efficiency in Estimation}

% Authors and affiliation
\author[hh]{Heath Henderson\corref{cauthor}}
\cortext[cauthor]{Corresponding author; 166D Heady Hall, Ames, IA 50011; 
Tel: +1 515 294 8122;  Email: \url{heathh@iastate.edu}.}
\author[ag]{Amos Golan}
\author[ss]{Skipper Seabold}

\address[hh]{Department of Economics, Iowa State University}
\address[ag]{Info-Metrics Institute and Department of Economics, 
American University}
\address[ss]{Department of Economics, American University}

% Abstract
\begin{abstract}
Prior information can improve inference, but in the social sciences observing such
information is quite rare.
We develop a new way to incorporate prior information within an 
Information-Theoretic (IT) estimation framework. 
The estimator considers a wide range of potential priors and then uses a simple statistic 
to choose the optimal model. 
The optimal model is the one where the informational distance between a certain prior and 
the solution is minimized.
Through a large number of sampling experiments, we demonstrate the small sample 
performance gains of our method relative to its classical competitors.
\end{abstract}

% Keywords and JEL codes
\begin{keyword}
Entropy \sep Generalized Cross Entropy  \sep Generalized Maximum Entropy 
\sep Information \sep Maximum Entropy \sep Priors \\
\textit{JEL codes}: C13 \sep C14  
\end{keyword}

\end{frontmatter}

\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Introduction %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec: intro}

Information-Theoretic (IT) methods of inference allow researchers to relax 
classical statistical assumptions in the estimation of both linear and non-linear models.
The Generalized Maximum Entropy (GME) estimator is an IT estimator that assumes 
minimal knowledge of statistical error structure and treats both signal and noise 
as unknown quantities. 
Thus, regardless of sample size the problem is always under-determined -- there are 
more unknown quantities than observed sample points. 
The Maximum Entropy (ME) formalism is a well-known approach for solving such problems
\citep{jaynes1957a, jaynes1957b}. 
The GME estimator builds on the ME formalism and uses the Boltzmann-Shannon entropy 
\citep{shannon1948} as the objective function.
When applied to ill-behaved problems, the GME estimator 
outperforms its traditional counterparts (e.g., ordinary least squares and maximum likelihood) 
for all finite samples and, as such, is used frequently in the social and natural sciences
\citep{golan1996}. 

The Generalized Cross Entropy (GCE) estimator generalizes GME by allowing 
researchers to incorporate prior information. 
The incorporation of prior information is done by substituting the Shannon entropy 
functional with the Kullback-Liebler divergence measure \citep{kullback1951}.  
Since the resulting problem is also under-determined, there are infinitely many solutions. 
Out of all possible solutions, the GCE estimator chooses the one that is closest to the 
chosen prior (in an informational sense).
The GME and GCE estimators are identical when uniform priors are used. 
However, when informative priors are used, the GCE estimator outperforms the GME
in terms of mean squared errors. 
Unfortunately, in the social sciences prior information is frequently missing. 
We ask if there is a way to uncover these priors.
If possible, we can improve the performance of the GCE estimator for all 
finite data. 

In this paper we propose a new approach for uncovering prior information. 
Since priors are unknown \emph{a priori}, we circumvent the issue by specifying a 
set of priors that covers (almost) all possible combinations. 
Incorporating this set of priors within a GCE framework and using the GCE criterion to 
select the ``optimal'' prior, we are able to improve the performance of the IT estimator 
relative to all other estimators and for all finite samples.
We demonstrate this via a large number of sampling experiments. 
Since the properties of the GCE estimator are already known \citep{golan1996, 
mittelhammer2000}, we concentrate on demonstrating small sample performance. 
Finally, while our approach is generic for all functional forms, we demonstrate it here 
only for the linear statistical model.

In Section \ref{sec: gce} we provide a brief summary of the GCE formalism and 
in Section \ref{sec: mp-gce} we outline our estimator. 
Section \ref{sec: mce} details the sampling experiments through which we 
compare the performance of our estimation strategy to the leading competing 
estimators. 
In Section \ref{sec: results} we describe our results and Section \ref{sec: conc}
concludes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% The GCE Estimator  %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Generalized Cross Entropy Estimator}
\label{sec: gce}

Consider the following linear regression model: 
\begin{equation}
\mathbf{y} = \mathbf{X\beta} + \mathbf{\varepsilon}
\label{eq: lrm}
\end{equation}

\noindent
where $\mathbf{y}$ is a $T$-dimensional vector of observations on the 
dependent variable, $\mathbf{X}$ is a $T\times K$ matrix of exogenous
variables, $\mathbf{\beta}$ is a $K$-dimensional vector of unknown 
parameters, and $\mathbf{\varepsilon}$ is a $T$-dimensional vector of 
random errors with mean zero.%
\footnote{Vectors are bolded and matrices are capitalized throughout.}
Each $\mathbf{\beta}_k$ and $\mathbf{\varepsilon}_t$ in the GCE 
framework is typically viewed as the mean value of some well-defined 
random variable, which we denote as $\mathbf{z}_k$ and $\mathbf{v}_t$, 
respectively.
Accordingly, let $\mathbf{p}_k$ be an $M$-dimensional proper 
probability distribution defined on the support $\mathbf{z}_k$ such that 
$\mathbf{\beta}_k = \sum_m p_{km}z_{km} = \mathbf{z}_k' 
\mathbf{p}_k$ where $\mathbf{z}_k'$ denotes the transpose of $\mathbf{z}_k$.
Similarly, $\mathbf{w}_t$ is a $J$-dimensional proper probability distribution defined on 
the symmetric about zero $\mathbf{v}_t$ such that $\mathbf{\varepsilon}_t = 
\sum_j w_{tj}v_{tj} = \mathbf{v}_t' \mathbf{w}_t$.

Eq.\ (\ref{eq: lrm}) can then be reparameterized as follows:
\begin{equation}
\mathbf{y} = \mathbf{X\beta} + \mathbf{\varepsilon} = 
\mathbf{X Z p} + \mathbf{V w}
\label{eq: reparm}
\end{equation}

\noindent
where $\mathbf{z}=[\mathbf{z}_1' ~ \mathbf{z}_2' ~ \cdots ~ 
\mathbf{z}_K' ]'$ and $\mathbf{v}=[\mathbf{v}_1' ~ \mathbf{v}_2' ~ 
\cdots ~ \mathbf{v}_T' ]'$, 
$\mathbf{Z}= (\mathbf{I}_K \otimes \mathbf{1}_M')\mathbf{z}$ and
$\mathbf{V}= (\mathbf{I}_T \otimes \mathbf{1}_J')\mathbf{v}$, $\mathbf{1}$ 
denotes a vector of ones, $\mathbf{I}$ is the 
identity matrix, and $\otimes$ is the Kronecker product.
Similarly, $\mathbf{p} = [\mathbf{p}_1' ~ \mathbf{p}_2' ~ \cdots ~ 
\mathbf{p}_K' ]'$ and $\mathbf{w} = [\mathbf{w}_1' ~ \mathbf{w}_2' 
~ \cdots ~ \mathbf{w}_T' ]'$.
The dimensions of $\mathbf{Z}$ and $\mathbf{V}$ are 
$K \times KM$ and $T \times TJ$, respectively, and the dimensions of 
$\mathbf{p}$ and $\mathbf{w}$ are $KM \times 1$ and $TJ \times 1$, 
respectively.%
\footnote{While it is possible to construct unbounded 
and continuous supports \citep{golan2012}, for the sake of simplicity the above 
support spaces are constructed as discrete and bounded.} 
We then estimate $\beta$ with minimal assumptions; treating the errors as an additional 
set of unknown quantities.

Let $\mathbf{q}$ be a $KM$-dimensional vector of prior weights for the 
parameters $\mathbf{\beta}$ with prior mean $\mathbf{Zq}$.
Analogously, let $\mathbf{u}$ be a $TJ$-dimensional vector of prior weights 
for the disturbances $\mathbf{\varepsilon}$ with prior mean $\mathbf{Vu}$.
The GCE estimator then selects $\mathbf{p}$, $\mathbf{w}$ $\gg$ 
$\mathbf{0}$ to minimize 
\begin{equation}
I({\mathbf{p}, \mathbf{q}, \mathbf{w}, \mathbf{u}}) = 
\mathbf{p}' \ln (\mathbf{p}/\mathbf{q}) + 
\mathbf{w}' \ln (\mathbf{w}/\mathbf{u})
\label{eq: ce}
\end{equation}

\noindent
subject to
\begin{equation}
\mathbf{y} = \mathbf{X Z p} 
+ \mathbf{V w}
\label{eq: glm}
\end{equation}
\begin{equation}
\mathbf{1}_K = (\mathbf{I}_K \otimes \mathbf{1}_M')\mathbf{p}
\label{eq: pp1}
\end{equation}
\begin{equation}
\mathbf{1}_T = (\mathbf{I}_T \otimes \mathbf{1}_J')\mathbf{w}
\label{eq: pp2}
\end{equation}

\noindent
where Eqs.\ (\ref{eq: glm}) are the data constraints and Eqs.\ 
(\ref{eq: pp1})-(\ref{eq: pp2}) are proper probability constraints.
See \citet[Chap.\ 6]{golan1996} or \citet[Chap.\ 6]{golan2008} for analytic solutions, 
discussion of efficient techniques for computation of the GCE solutions via the 
unconstrained dual or concentrated version of the problem, and issues of inference.
Finally, note that with uniform priors the GCE estimator is identical to 
the GME estimator, which instead maximizes $H(\mathbf{p},\mathbf{w}) 
= - \mathbf{p}' \ln (\mathbf{p}) - \mathbf{w}' \ln (\mathbf{w})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% The MP-GCE Estimator  %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multi-Prior Generalized Cross Entropy Estimator}
\label{sec: mp-gce}

As with all problems of inference, the choice of priors remains a central question.
However, for many applications across the social sciences prior information 
does not exist. 
While researchers typically select uniform, or otherwise uninformative, priors in these 
situations, more informative priors can significantly improve small sample performance.
One way to overcome the problem empirically is to specify a set of all (or 
nearly all) possible priors and utilize an appropriate criterion to choose one of 
the solutions.
In what follows we elaborate on this multi-prior GCE estimator (MP-GCE).

Given a set of priors, we need to choose a rule that selects the best prior that is 
consistent with our observed sample. 
Our MP-GCE estimator minimizes Eq.\ (\ref{eq: ce}) subject to 
Eqs.\ (\ref{eq: glm})-(\ref{eq: pp2}) for (potentially many) alternative priors and then 
selects the model for which $I(\cdot)$ is itself minimized. 
Stated differently, there are as many ``optimal'' solutions as priors and, following the logic 
of information theory, we choose the solution that minimizes the Kullback-Leibler criterion 
$I(\cdot)$. 
This is the solution where the \emph{a priori} unknown prior is closest to the observed 
data and, as such, the MP-GCE estimator identifies the ``best'' prior out of the 
set of potential priors. 
While computationally intensive, we demonstrate below that a relatively small 
number of priors is sufficient for the MP-GCE estimator to outperform its 
traditional counterparts.

The question remains as to which alternative priors to incorporate.
Let $\mathbf{q}=[\mathbf{q}_1' ~ \mathbf{q}_2' ~ \cdots ~ \mathbf{q}_K' ]'$ where
$\mathbf{q}_k$ is an $M$-dimensional vector of prior weights for the parameter 
$\beta_k$.
For the sake of clarity, we focus on $\mathbf{q}_2$ and assume $\mathbf{q}_k$ to 
be uniform for $k=1,3,4,\ldots, K$.
In developing candidate priors, we define proper probabilities on the vector 
$\mathbf{z}_2$ such that across alternative choices of $\mathbf{q}_2$ the 
expectation $\mathbf{z}_2'\mathbf{q}_2$  yields values that appropriately
span the support space. 
Let $S$ represent all  $M!$ possible permutations of 
$s = [1^\rho ~ 2^\rho ~ \cdots ~ M^\rho]$.
Normalizing each element of $S$ by $\sum_{m=1}^M m^\rho$, it is 
readily verified that we have $M!$ candidate $M$-dimensional vectors for 
$\mathbf{q}_2$ that span the support space.%
\footnote{The parameter $\rho \in [-\infty, \infty]$ permits the researcher
to control the dispersion of the alternative prior means about the center of the 
support space.
Notably, as $\rho \to 0$ all priors approach uniform and as $|\rho| \to 
\infty$ the prior means approach the boundaries of the support space.}
Incorporating the uniform prior $[1/M ~ 1/M ~ \cdots ~ 1/M]$, we have a total
of $M! + 1$ priors.
The MP-GCE estimator then minimizes Eq.\ (\ref{eq: ce}) subject 
to Eqs.\ (\ref{eq: glm})-(\ref{eq: pp2}) after inserting into $\mathbf{q}$ 
each of the $M! + 1$ priors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% Sampling Experiments %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sampling Experiments}
\label{sec: mce}

For each replication in the sampling experiments our objective is to minimize 
$I(\cdot)$ subject to the data consistency and proper probability constraints 
for alternative choices of $\mathbf{q}$. 
To measure the performance of each prior we focus on the mean squared 
error MSE$(\hat{\mathbf{\beta}}) = E \| \hat{\mathbf{\beta}}- 
\mathbf{\beta}\|^2$.
To establish a basis for selecting among alternative priors, the relationship 
between MSE$(\hat{\mathbf{\beta}})$ and $E[I(\cdot)]$ is examined.
This relationship is considered for different sample sizes, levels of noise, 
and degrees of correlation among covariates.
Further, we include the performance of the OLS and GME estimators for 
comparison with the MP-GCE estimator.

Consider a baseline scenario where $\mathbf{X}$ in Eq.\ (\ref{eq: reparm})
is a $10 \times 3$ design matrix where $\mathbf{x}_k \sim 
N(0,1)$, $\mathbf{\beta}= [\beta_1 ~ \beta_2 ~ \beta_3]'=[1 ~ -3 ~ 2]'$, 
and $\mathbf{\varepsilon} \sim N(0,1)$.
In order to control correlation among covariates, the condition
number $\kappa(\mathbf{X}'\mathbf{X})=1$ is specified by appropriately replacing the 
eigenvalues in the singular value decomposition of $\mathbf{X}$.%
\footnote{See \citet[pg.\ 133]{golan1996} and references therein for 
details.}
For $M=J=3$ points, the support vector for $\mathbf{p}_k$ is 
$\mathbf{z}_k = [- 10 ~ 0 ~ 10]'$ and the support vector for $\mathbf{w}_t$ 
is $\mathbf{v}_t = [- 3\sigma_y ~ 0 ~ 3\sigma_y]'$ where $\sigma_y$ is the sample
standard deviation of $\mathbf{y}$.%
\footnote{The error support vector is based on the three-sigma
rule \citep{pukelsheim1994} and is calculated 
uniquely for each replication based on the observed $\mathbf{y}$.}
Let $\mathbf{q} = [\mathbf{q}_1' ~ 
\mathbf{q}_2' ~ \mathbf{q}_3']'$ represent prior information on the 
respective elements of the parameter vector.
As in Section \ref{sec: mp-gce}, we let $\mathbf{q}_1 = \mathbf{q}_3 = 1/M 
= 1/3$ (i.e., uniform) and estimate the model using $\rho=0.5$  in the 
discussed alternative specifications of $\mathbf{q}_2$ denoted in Table \ref{tbl: priors}.
We use uniform priors on the error term throughout (i.e., $\mathbf{u} = 1/J = 1/3$).
Finally, the experiment is conducted with $N=1,000$ replications.

% Priors
\footnotesize
\ctable[
cap = {Priors for Experiments},
caption = {Alternative priors used in sampling experiments.\ $\mathbf{q}_2$ 
denotes the vector of priors associated with the parameter $\beta_2$.},
captionskip = -2ex,
pos=htb,
label = {tbl: priors}
]{lccc}{
}{\hline \hline
Label & $\mathbf{q_2}$  \\ \hline
1 & $[\sqrt{1} ~ \sqrt{2} ~ \sqrt{3}]'/\sum_{m=1}^3 \sqrt{m}$\\    
2 & $[\sqrt{1} ~ \sqrt{3} ~ \sqrt{2}]'/\sum_{m=1}^3 \sqrt{m}$ \\
3 & $[\sqrt{2} ~ \sqrt{1} ~ \sqrt{3}]'/\sum_{m=1}^3 \sqrt{m}$\\
4 & $[\sqrt{2} ~ \sqrt{3} ~ \sqrt{1}]'/\sum_{m=1}^3 \sqrt{m}$\\
5 & $[\sqrt{3} ~ \sqrt{1} ~ \sqrt{2}]'/\sum_{m=1}^3 \sqrt{m}$\\
6 & $[\sqrt{3} ~ \sqrt{2} ~ \sqrt{1}]'/\sum_{m=1}^3 \sqrt{m}$\\
7 & $[\sqrt{1/3} ~ \sqrt{1/3} ~ \sqrt{1/3}]/\sum_{m=1}^3 \sqrt{m}$ \\\hline}
\normalsize

As mentioned, we vary the sample sizes, the level of noise, and the degree of 
correlation among covariates in the experiments.
Beyond $T=10$ we consider sample sizes of $T=20, 50, 100, \text{and } 200$. 
We increase the level of noise in one set of experiments to  
$\mathbf{\varepsilon} \sim N(0,5)$.
Finally, we let $\kappa(\mathbf{X}'\mathbf{X})=100$, which represents a moderately 
ill-conditioned design matrix.
This gives three sets of experiments that we label well-behaved, noisy, and collinear, 
each of which considers five different sample sizes.
Table \ref{tbl: exp} summarizes these experiments.

% Sampling Experiments
\footnotesize
\ctable[
cap = {Sampling Experiments},
caption = {Summary of sampling experiments conducted.\  The columns marked $T$, 
$\varepsilon$, and $\kappa( \mathbf{X'X})$ display
the sample size, error specification, and condition number, respectively.},
captionskip = -2ex,
pos=htb,
label = {tbl: exp}
]{lccc}{
}{\hline \hline
Experiment Set & \multicolumn{1}{c}{$T$} & 
\multicolumn{1}{c}{$\varepsilon$} &
\multicolumn{1}{c}{$\kappa(\mathbf{X}'\mathbf{X})$} \\ \hline
\multirow{5}{*}{Well-Behaved} & 10 & $N(0,1)$ &   1   \\ 
& 20 & $N(0,1)$ &   1    \\ 
& 50 & $N(0,1)$ &   1    \\ 
& 100 & $N(0,1)$ &   1    \\ 
& 200 & $N(0,1)$ &   1    \\ \hline
\multirow{5}{*}{Noisy} & 10 & $N(0,5)$ &   1   \\ 
& 20 & $N(0,5)$ &   1   \\
& 50 & $N(0,5)$ &   1   \\  
& 100 & $N(0,5)$ &   1   \\
& 200 & $N(0,5)$ &   1   \\  \hline
\multirow{5}{*}{Collinear} & 10 & $N(0,1)$ & 100   \\
& 20 & $N(0,1)$ & 100   \\ 
& 50 & $N(0,1)$ & 100   \\ 
& 100 & $N(0,1)$ & 100   \\ 
& 200 & $N(0,1)$ & 100   \\    \hline}
\normalsize

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Results %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{sec: results}

Figures \ref{fig: B1}-\ref{fig: MSEB} present the results of the sampling experiments.
For the baseline scenario, Figure \ref{fig: B1} plots the replication-average of 
$I(\mathbf{p_2}, \mathbf{q_2}) = \mathbf{p_2}'\ln (\mathbf{p_2}/
\mathbf{q_2})$ (i.e., the cross entropy associated with $\beta_2$) on 
MSE$(\hat{\beta}_2)$ for alternative choices of $\mathbf{q_2}$.
Figure \ref{fig: B} instead plots the replication-average of 
$I(\mathbf{p}, \mathbf{q}) = \mathbf{p}'\ln (\mathbf{p}/
\mathbf{q})$ (i.e., the cross entropy associated with $\mathbf{\beta}$) on 
MSE$(\hat{\beta})$  for alternative choices of $\mathbf{q_2}$.
Table \ref{tbl: priors} provides the prior that corresponds to each label.
These figures depict our first core result.
We find a strong positive relationship between $I(\cdot)$ and MSE in both cases.
While we only consider alternative choices of $\mathbf{q_2}$, this result 
confirms that choosing the prior that minimizes the appropriate variant of $I(\cdot)$ will 
not only improve efficiency in the estimation of $\beta_2$, but also $\mathbf{\beta}$.

% B1 figure
\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{B2.pdf}
\caption{Cross entropy $I(\mathbf{p_2}, \mathbf{q_2})$ vs.\  mean squared
error of $\hat{\beta}_2$ for the alternative priors used in the baseline experiment.
Prior labels are provided in Table \ref{tbl: priors}.
The strong positive relationship suggests using $I(\mathbf{p_2}, \mathbf{q_2})$
as a criterion  for prior selection to increase efficiency in estimation of 
$\beta_2$.}
\label{fig: B1}
\end{figure}

% B figure
\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{B.pdf}
\caption{Cross entropy $I(\mathbf{p}, \mathbf{q})$ vs.\  mean squared
error of $\mathbf{\hat{\beta}}$ for the alternative priors used in the baseline experiment.
Prior labels are provided in Table \ref{tbl: priors}.
The strong positive relationship suggests using $I(\mathbf{p}, \mathbf{q})$
as a criterion for prior selection to increase efficiency in estimation of 
$\mathbf{\beta}$.}
\label{fig: B}
\end{figure}

Figure \ref{fig: MSEB1} plots MSE($\hat{\beta}_2$) on sample size for each 
experiment set (i.e., well-behaved, noisy, and collinear) and estimator (i.e., MP-GCE, 
GME, and OLS).
Figure \ref{fig: MSEB} is identical to Figure \ref{fig: MSEB1} except that it plots 
MSE($\mathbf{\hat{\beta}}$) instead of MSE($\hat{\beta}_2$).
The prior used by MP-GCE in Figures \ref{fig: MSEB1} and \ref{fig: MSEB} is that which 
minimizes $I(\mathbf{p_2}, \mathbf{q_2})$ and $I(\mathbf{p}, \mathbf{q})$, respectively.
The figures are qualitatively similar and demonstrate a second core result.
In each experiment MP-GCE outperforms GME and GME outperforms OLS in terms of MSE.
The efficiency gains are non-negligible.
For example, for the baseline scenario in Figure \ref{fig: MSEB1} GME reduces MSE by 
19 percent relative to OLS and MP-GCE reduces MSE by 16 percent relative to GME.
Naturally, the estimators converge in MSE as sample size increases, but in some cases
(e.g., the noisy case) the efficiency gains persist even for larger sample sizes.

% MSEB1 figure
\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{MSEB2.pdf}
\caption{Mean squared error of $\hat{\beta}_2$ vs.\ sample size for the well-behaved,
noisy, and collinear experiments. The results demonstrate that MP-GCE outperforms 
GME and GME outperforms OLS in terms of the mean squared error of $\hat{\beta}_2$.}
\label{fig: MSEB1}
\end{figure}

% MSEB figure
\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{MSEB.pdf}
\caption{Mean squared error of $\hat{\beta}$ vs.\ sample size for the well-behaved,
noisy, and collinear experiments. The results demonstrate that MP-GCE outperforms 
GME and GME outperforms OLS in terms of the mean squared error of $\hat{\beta}$.}
\label{fig: MSEB}
\end{figure}

We conclude this section with a brief discussion of additional sampling 
experiments conducted, which included increasing the number of regressors,
doubling the parameter vector, and incorporating alternative priors on $\beta_3$.
The most notable of these experiments is the incorporation of alternative priors.
In combination with the $M! + 1$ priors on $\beta_2$, we considered the 
same $M! + 1$ priors on $\beta_3$.%
\footnote{Thus, for $M=3$, these experiments used a total of $(M! + 1)\times (M! + 1) 
= 49$ alternative priors.}
We found only modest MSE reductions for the baseline and noisy cases, but
non-negligible MSE reductions for the collinear case.
For example, MP-GCE witnessed MSE$(\mathbf{\hat{\beta}})=7.85$ for $T=10$,
which represents a 38 percent MSE reduction relative to the MP-GCE results presented
in Figure \ref{fig: MSEB}.
Thus, further efficiency gains can be made by considering additional priors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Conclusions %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec: conc}

We proposed a new approach for incorporating a whole class of priors when the true 
priors are unknown. 
Incorporating this within an IT framework, we have shown that our MP-GCE estimator 
outperforms its competitors for all finite data. 
This new approach is easy to use and can be used with linear or non-linear IT methods. 
Since this method is a generalization of the GCE estimator, all inferential statistics and 
large sample properties are already well known. 
We thus concentrated here on the increase in small sample efficiency, particularly for
the linear statistical model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% References %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Start fresh page
\newpage
\cleardoublepage
\singlespacing

%Declare the style and file to use
\section*{References}
\bibliographystyle{model5-names}\biboptions{authoryear}
\bibliography{/Users/hendersonhl/Documents/References}

\end{document}